{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333ae546d607a744",
   "metadata": {},
   "source": [
    "# Workshop: Building an Information Retrieval System for Podcast Episodes\n",
    "\n",
    "## Objective:\n",
    "Create an Information Retrieval (IR) system that processes a dataset of podcast transcripts and, given a query, returns the episodes where the host and guest discuss the query topic. Use TF-IDF and BERT for vector space representation and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecbddbf",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "### Step 1: Import Libraries\n",
    "Import necessary libraries for data handling, text processing, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c2cfa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c44fd",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset\n",
    "\n",
    "Load the dataset of podcast transcripts.\n",
    "\n",
    "Find the dataset in: https://www.kaggle.com/datasets/rajneesh231/lex-fridman-podcast-transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5afb84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'data/podcastdata_dataset.csv'\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(csv_file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b209fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guest</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Max Tegmark</td>\n",
       "      <td>Life 3.0</td>\n",
       "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Christof Koch</td>\n",
       "      <td>Consciousness</td>\n",
       "      <td>As part of MIT course 6S099 on artificial gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Steven Pinker</td>\n",
       "      <td>AI in the Age of Reason</td>\n",
       "      <td>You've studied the human mind, cognition, lang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yoshua Bengio</td>\n",
       "      <td>Deep Learning</td>\n",
       "      <td>What difference between biological neural netw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vladimir Vapnik</td>\n",
       "      <td>Statistical Learning</td>\n",
       "      <td>The following is a conversation with Vladimir ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Ray Kurzweil</td>\n",
       "      <td>Singularity, Superintelligence, and Immortality</td>\n",
       "      <td>By the time he gets to 2045, we'll be able to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>Rana el Kaliouby</td>\n",
       "      <td>Emotion AI, Social Robots, and Self-Driving Cars</td>\n",
       "      <td>there's a broader question here, right? As we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>Will Sasso</td>\n",
       "      <td>Comedy, MADtv, AI, Friendship, Madness, and Pr...</td>\n",
       "      <td>Once this whole thing falls apart and we are c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>Daniel Negreanu</td>\n",
       "      <td>Poker</td>\n",
       "      <td>you could be the seventh best player in the wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>Michael Levin</td>\n",
       "      <td>Biology, Life, Aliens, Evolution, Embryogenesi...</td>\n",
       "      <td>turns out that if you train a planarian and th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                guest                                              title  \\\n",
       "id                                                                         \n",
       "1         Max Tegmark                                           Life 3.0   \n",
       "2       Christof Koch                                      Consciousness   \n",
       "3       Steven Pinker                            AI in the Age of Reason   \n",
       "4       Yoshua Bengio                                      Deep Learning   \n",
       "5     Vladimir Vapnik                               Statistical Learning   \n",
       "..                ...                                                ...   \n",
       "321      Ray Kurzweil    Singularity, Superintelligence, and Immortality   \n",
       "322  Rana el Kaliouby   Emotion AI, Social Robots, and Self-Driving Cars   \n",
       "323        Will Sasso  Comedy, MADtv, AI, Friendship, Madness, and Pr...   \n",
       "324   Daniel Negreanu                                              Poker   \n",
       "325     Michael Levin  Biology, Life, Aliens, Evolution, Embryogenesi...   \n",
       "\n",
       "                                                  text  \n",
       "id                                                      \n",
       "1    As part of MIT course 6S099, Artificial Genera...  \n",
       "2    As part of MIT course 6S099 on artificial gene...  \n",
       "3    You've studied the human mind, cognition, lang...  \n",
       "4    What difference between biological neural netw...  \n",
       "5    The following is a conversation with Vladimir ...  \n",
       "..                                                 ...  \n",
       "321  By the time he gets to 2045, we'll be able to ...  \n",
       "322  there's a broader question here, right? As we ...  \n",
       "323  Once this whole thing falls apart and we are c...  \n",
       "324  you could be the seventh best player in the wh...  \n",
       "325  turns out that if you train a planarian and th...  \n",
       "\n",
       "[319 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ea2f1",
   "metadata": {},
   "source": [
    "### Step 3: Text Preprocessing\n",
    "\n",
    "You know what to do ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd6c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_file = 'data/stopwords.txt'\n",
    "\n",
    "with open(stop_words_file, 'r') as file:\n",
    "    stop_words = set(file.read().split())\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c99f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_lemmatize_text(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Eliminar caracteres especiales\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenizar el texto\n",
    "    words = text.split()\n",
    "    \n",
    "    # Eliminar stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Unir las palabras procesadas de nuevo en una sola cadena\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    # Lematizar el texto usando spaCy\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "    \n",
    "    return lemmatized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43eeeb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['text'].apply(preprocess_and_lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84be1ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guest</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Max Tegmark</td>\n",
       "      <td>Life 3.0</td>\n",
       "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
       "      <td>part mit 6s099 artificial general intelligence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Christof Koch</td>\n",
       "      <td>Consciousness</td>\n",
       "      <td>As part of MIT course 6S099 on artificial gene...</td>\n",
       "      <td>part mit 6s099 artificial general intelligence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Steven Pinker</td>\n",
       "      <td>AI in the Age of Reason</td>\n",
       "      <td>You've studied the human mind, cognition, lang...</td>\n",
       "      <td>you ve study human mind cognition language vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yoshua Bengio</td>\n",
       "      <td>Deep Learning</td>\n",
       "      <td>What difference between biological neural netw...</td>\n",
       "      <td>difference biological neural network artificia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vladimir Vapnik</td>\n",
       "      <td>Statistical Learning</td>\n",
       "      <td>The following is a conversation with Vladimir ...</td>\n",
       "      <td>conversation vladimir vapnik he s inventor sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Ray Kurzweil</td>\n",
       "      <td>Singularity, Superintelligence, and Immortality</td>\n",
       "      <td>By the time he gets to 2045, we'll be able to ...</td>\n",
       "      <td>time 2045 multiply intelligence million fold h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>Rana el Kaliouby</td>\n",
       "      <td>Emotion AI, Social Robots, and Self-Driving Cars</td>\n",
       "      <td>there's a broader question here, right? As we ...</td>\n",
       "      <td>broad question build socially emotionally inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>Will Sasso</td>\n",
       "      <td>Comedy, MADtv, AI, Friendship, Madness, and Pr...</td>\n",
       "      <td>Once this whole thing falls apart and we are c...</td>\n",
       "      <td>thing fall climb kudzu vine spiral sears tower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>Daniel Negreanu</td>\n",
       "      <td>Poker</td>\n",
       "      <td>you could be the seventh best player in the wh...</td>\n",
       "      <td>seventh player world literally seventh player ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>Michael Levin</td>\n",
       "      <td>Biology, Life, Aliens, Evolution, Embryogenesi...</td>\n",
       "      <td>turns out that if you train a planarian and th...</td>\n",
       "      <td>turn train planarian cut head tail regenerate ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                guest                                              title  \\\n",
       "id                                                                         \n",
       "1         Max Tegmark                                           Life 3.0   \n",
       "2       Christof Koch                                      Consciousness   \n",
       "3       Steven Pinker                            AI in the Age of Reason   \n",
       "4       Yoshua Bengio                                      Deep Learning   \n",
       "5     Vladimir Vapnik                               Statistical Learning   \n",
       "..                ...                                                ...   \n",
       "321      Ray Kurzweil    Singularity, Superintelligence, and Immortality   \n",
       "322  Rana el Kaliouby   Emotion AI, Social Robots, and Self-Driving Cars   \n",
       "323        Will Sasso  Comedy, MADtv, AI, Friendship, Madness, and Pr...   \n",
       "324   Daniel Negreanu                                              Poker   \n",
       "325     Michael Levin  Biology, Life, Aliens, Evolution, Embryogenesi...   \n",
       "\n",
       "                                                  text  \\\n",
       "id                                                       \n",
       "1    As part of MIT course 6S099, Artificial Genera...   \n",
       "2    As part of MIT course 6S099 on artificial gene...   \n",
       "3    You've studied the human mind, cognition, lang...   \n",
       "4    What difference between biological neural netw...   \n",
       "5    The following is a conversation with Vladimir ...   \n",
       "..                                                 ...   \n",
       "321  By the time he gets to 2045, we'll be able to ...   \n",
       "322  there's a broader question here, right? As we ...   \n",
       "323  Once this whole thing falls apart and we are c...   \n",
       "324  you could be the seventh best player in the wh...   \n",
       "325  turns out that if you train a planarian and th...   \n",
       "\n",
       "                                          cleaned_text  \n",
       "id                                                      \n",
       "1    part mit 6s099 artificial general intelligence...  \n",
       "2    part mit 6s099 artificial general intelligence...  \n",
       "3    you ve study human mind cognition language vis...  \n",
       "4    difference biological neural network artificia...  \n",
       "5    conversation vladimir vapnik he s inventor sup...  \n",
       "..                                                 ...  \n",
       "321  time 2045 multiply intelligence million fold h...  \n",
       "322  broad question build socially emotionally inte...  \n",
       "323  thing fall climb kudzu vine spiral sears tower...  \n",
       "324  seventh player world literally seventh player ...  \n",
       "325  turn train planarian cut head tail regenerate ...  \n",
       "\n",
       "[319 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df666795",
   "metadata": {},
   "source": [
    "###  Step 4: Vector Space Representation - TF-IDF\n",
    "\n",
    "Create TF-IDF vector representations of the transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d6413d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para vectorizar textos utilizando TF-IDF\n",
    "def TF_IDF(texts):\n",
    "    # Vectorización usando TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "    \n",
    "    return X_tfidf, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95181122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Lemmatized:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Caracteristicas de TF-IDF Lemmatized: ['00' '000' '00000' ... 'это' 'этот' '들어가']\n"
     ]
    }
   ],
   "source": [
    "# Vectorizar los documentos\n",
    "X_tfidf_lemmatized, tfidf_vectorizer_lemmatized = TF_IDF(df['cleaned_text'])\n",
    "\n",
    "# Ver los resultados de TF-IDF\n",
    "print(\"TF-IDF Lemmatized:\")\n",
    "print(X_tfidf_lemmatized.toarray())\n",
    "print(\"Caracteristicas de TF-IDF Lemmatized:\", tfidf_vectorizer_lemmatized.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d0a28",
   "metadata": {},
   "source": [
    "### Step 5: Vector Space Representation - BERT\n",
    "\n",
    "Create BERT vector representations of the transcripts using a pre-trained BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c2c6ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd4ce121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Embeddings: [[[ 0.06086778]\n",
      "  [ 0.26899973]\n",
      "  [ 0.37482524]\n",
      "  ...\n",
      "  [-0.24746999]\n",
      "  [ 0.11418366]\n",
      "  [ 0.12912685]]\n",
      "\n",
      " [[-0.03892427]\n",
      "  [ 0.1501886 ]\n",
      "  [ 0.18922803]\n",
      "  ...\n",
      "  [-0.33711198]\n",
      "  [ 0.13962337]\n",
      "  [ 0.08770353]]\n",
      "\n",
      " [[-0.15095787]\n",
      "  [ 0.06957125]\n",
      "  [ 0.15182185]\n",
      "  ...\n",
      "  [-0.3761814 ]\n",
      "  [ 0.16042688]\n",
      "  [ 0.12493543]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.01123998]\n",
      "  [ 0.02063517]\n",
      "  [ 0.4532022 ]\n",
      "  ...\n",
      "  [-0.1953187 ]\n",
      "  [ 0.19546221]\n",
      "  [-0.07874274]]\n",
      "\n",
      " [[-0.02113011]\n",
      "  [ 0.28063643]\n",
      "  [ 0.16844061]\n",
      "  ...\n",
      "  [-0.25958553]\n",
      "  [ 0.19384375]\n",
      "  [ 0.12385748]]\n",
      "\n",
      " [[-0.03289397]\n",
      "  [ 0.21554986]\n",
      "  [ 0.25829577]\n",
      "  ...\n",
      "  [-0.28611344]\n",
      "  [ 0.07276228]\n",
      "  [ 0.19506222]]]\n",
      "BERT Shape: (319, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "def generate_bert_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True)\n",
    "        outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :])  # Use [CLS] token representation\n",
    "    return np.array(embeddings).transpose(0,2,1)\n",
    "\n",
    "bert_embeddings = generate_bert_embeddings(df['cleaned_text'])\n",
    "print(\"BERT Embeddings:\", bert_embeddings)\n",
    "print(\"BERT Shape:\", bert_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c0385",
   "metadata": {},
   "source": [
    "### Step 6: Query Processing\n",
    "\n",
    "Define a function to process the query and compute similarity scores using both TF-IDF and BERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5580c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Stack Overflow and Coding Horror'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67b8b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query(query):\n",
    "    # Limpiamos la query\n",
    "    query = query.lower()\n",
    "    stop_words_file = 'data/stopwords.txt'\n",
    "    with open(stop_words_file, 'r', encoding='utf-8') as file:\n",
    "        stop_words = set(file.read().split())\n",
    "\n",
    "    cleaned_query = ' '.join([word for word in query.split() if word not in stop_words])\n",
    "    cleaned_query = re.sub(r'[^A-Za-z0-9\\s]', '', cleaned_query)\n",
    "    return cleaned_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c1ef668",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_query = clean_query(query)\n",
    "# lematizamos y stematizamos la query\n",
    "\n",
    "lemmatized_query = preprocess_and_lemmatize_text(cleaned_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe710350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar Query usando tdidf lemmatized\n",
    "\n",
    "query_tfidf_lemmatized = tfidf_vectorizer_lemmatized.transform([lemmatized_query])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00631a7a",
   "metadata": {},
   "source": [
    "### Step 7: Retrieve and Compare Results\n",
    "\n",
    "Define a function to retrieve the top results based on similarity scores for both TF-IDF and BERT representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14668b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_documents(scores, n, filenames):\n",
    "\n",
    "    # Asegurarse de que los puntajes sean un array 1D\n",
    "    scores = scores.flatten()\n",
    "\n",
    "    # Obtener los índices de los n puntajes más altos\n",
    "    top_indices = np.argsort(scores)[-n:][::-1]\n",
    "\n",
    "    # Obtener los textos correspondientes a esos índices\n",
    "\n",
    "    top_texts = []\n",
    "    top_titles = []\n",
    "\n",
    "    for idx in top_indices:\n",
    "        top_texts.append(df['title'])\n",
    "\n",
    "    top_scores = scores[top_indices]\n",
    "\n",
    "    # Imprimir los 10 textos con mayor similitud y sus puntajes\n",
    "    for i, (text, title, score) in enumerate(zip(top_texts, top_titles, top_scores), 1):\n",
    "\n",
    "        print(f\"Top {i}: {title} - Similitud: {score:.4f}\")\n",
    "        print(text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57714542",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_scores_lemmatized = cosine_similarity(query_tfidf_lemmatized, X_tfidf_lemmatized)\n",
    "\n",
    "top_n_documents(cosine_scores_lemmatized, 5, df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2518925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.05121997e-03, 1.14090344e-03, 1.80429485e-03, 0.00000000e+00,\n",
       "        1.07825069e-03, 2.16699391e-02, 3.34643049e-01, 0.00000000e+00,\n",
       "        1.05902109e-03, 1.43089768e-03, 8.62788215e-03, 2.31556940e-03,\n",
       "        0.00000000e+00, 1.06194520e-02, 1.06194520e-02, 1.08095882e-03,\n",
       "        5.34781287e-03, 1.54863843e-03, 3.78983961e-03, 3.54187641e-03,\n",
       "        2.54097295e-03, 5.08895533e-02, 3.78679696e-03, 3.93808673e-03,\n",
       "        0.00000000e+00, 1.80996223e-03, 0.00000000e+00, 9.61147734e-04,\n",
       "        0.00000000e+00, 7.22093153e-03, 3.66884015e-03, 3.14403967e-02,\n",
       "        6.53510324e-03, 7.58434337e-04, 0.00000000e+00, 1.10269175e-02,\n",
       "        5.56657524e-03, 0.00000000e+00, 7.44890564e-03, 0.00000000e+00,\n",
       "        1.06385618e-03, 0.00000000e+00, 2.04376890e-02, 8.87238662e-03,\n",
       "        1.12602945e-03, 0.00000000e+00, 0.00000000e+00, 1.68912727e-03,\n",
       "        8.88285587e-02, 0.00000000e+00, 1.73784058e-03, 3.42098844e-03,\n",
       "        2.24740084e-03, 4.13562372e-03, 1.79240811e-03, 1.40099056e-02,\n",
       "        3.25562894e-03, 1.97411722e-03, 2.59488117e-03, 7.82896059e-03,\n",
       "        1.07360964e-02, 3.93714319e-03, 5.12496248e-03, 1.42443203e-03,\n",
       "        6.32890950e-03, 2.36174164e-03, 7.32897460e-03, 2.47261837e-03,\n",
       "        3.44491777e-03, 5.20810645e-03, 2.44391545e-02, 1.37244804e-03,\n",
       "        6.69783366e-03, 2.24548707e-02, 1.77846289e-03, 2.18417612e-03,\n",
       "        5.88885000e-03, 4.35125289e-03, 3.59418613e-03, 3.37112690e-03,\n",
       "        4.27158036e-03, 2.11149527e-03, 6.80002170e-03, 3.88381791e-03,\n",
       "        2.38769403e-03, 4.23622471e-03, 2.71426322e-03, 7.89124882e-03,\n",
       "        3.25273154e-03, 4.03284299e-03, 2.72287946e-03, 0.00000000e+00,\n",
       "        1.66298765e-03, 2.88153253e-03, 8.47810173e-03, 2.54708064e-02,\n",
       "        1.30592325e-03, 1.03120427e-02, 6.89466383e-03, 4.21273268e-03,\n",
       "        5.39956779e-03, 9.73779017e-03, 3.67492132e-03, 2.67260769e-02,\n",
       "        4.17510630e-03, 7.18408770e-03, 4.82753424e-03, 4.01938848e-03,\n",
       "        1.28845805e-02, 2.79136065e-03, 2.94996084e-03, 3.96290081e-03,\n",
       "        8.89124470e-03, 5.81489207e-03, 4.36538211e-03, 3.77213567e-03,\n",
       "        3.92255026e-03, 6.00692264e-03, 4.98383935e-03, 4.40554933e-03,\n",
       "        2.39649730e-03, 1.59477081e-03, 5.46728682e-03, 4.35502760e-03,\n",
       "        4.67703479e-03, 3.41941076e-02, 0.00000000e+00, 2.62208467e-03,\n",
       "        0.00000000e+00, 7.64459602e-03, 2.90004252e-02, 1.22968662e-02,\n",
       "        1.83288248e-03, 4.93128081e-03, 3.10000845e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.91898869e-03, 6.14577332e-03, 0.00000000e+00,\n",
       "        4.19487133e-03, 4.34749377e-03, 3.87637008e-04, 5.79043078e-03,\n",
       "        7.12905109e-03, 3.49478143e-03, 1.46534004e-02, 5.65663748e-03,\n",
       "        0.00000000e+00, 2.64697200e-03, 1.41527462e-03, 0.00000000e+00,\n",
       "        2.41956177e-03, 0.00000000e+00, 1.57897303e-03, 2.70233411e-03,\n",
       "        7.47237883e-04, 5.28230773e-03, 4.18514341e-03, 1.78936397e-02,\n",
       "        4.37087722e-03, 1.86577604e-02, 1.57026840e-03, 3.21164475e-03,\n",
       "        1.08610328e-02, 7.91878912e-03, 2.31973768e-03, 3.15964494e-03,\n",
       "        3.06677653e-03, 0.00000000e+00, 1.34599093e-03, 1.94475711e-02,\n",
       "        3.49589285e-03, 0.00000000e+00, 2.67097867e-03, 2.13171258e-03,\n",
       "        4.44358226e-03, 8.04811348e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.43443881e-02, 4.63776659e-03, 2.81772459e-03, 0.00000000e+00,\n",
       "        4.62507247e-04, 1.23933590e-02, 1.32411361e-03, 5.89434618e-03,\n",
       "        7.55267411e-04, 7.11308563e-03, 2.88749224e-03, 2.23527144e-02,\n",
       "        2.61078243e-03, 1.61306554e-02, 1.45143661e-02, 0.00000000e+00,\n",
       "        1.73949683e-03, 1.08573194e-03, 0.00000000e+00, 1.05122874e-02,\n",
       "        3.25557375e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.84984738e-03, 1.93688236e-03, 3.79522455e-03, 2.24499526e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.77427897e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.59929489e-02, 9.54103473e-04, 1.25324048e-03,\n",
       "        7.30358071e-04, 4.93253268e-02, 3.80183537e-03, 4.17506055e-03,\n",
       "        2.03628796e-03, 0.00000000e+00, 2.57886867e-02, 1.03780026e-02,\n",
       "        2.77731001e-03, 0.00000000e+00, 4.16163607e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.69076963e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.77095578e-03, 9.99012424e-04, 0.00000000e+00, 3.28811535e-03,\n",
       "        1.71351449e-03, 0.00000000e+00, 1.12107496e-03, 1.62054377e-02,\n",
       "        1.08327246e-03, 4.41758754e-03, 1.88888921e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.34257367e-03, 2.79320939e-03, 0.00000000e+00,\n",
       "        2.57074619e-02, 5.26762157e-03, 1.65360721e-02, 1.38472046e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.37520120e-03, 2.30193389e-03,\n",
       "        5.52584558e-03, 1.76512580e-03, 0.00000000e+00, 1.19266328e-03,\n",
       "        3.65192865e-03, 5.43857575e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.41064842e-03, 8.12331774e-03, 8.48486888e-03, 0.00000000e+00,\n",
       "        8.01413156e-03, 2.48330342e-03, 3.05978181e-03, 4.46679238e-04,\n",
       "        0.00000000e+00, 1.95276681e-03, 3.78176666e-04, 0.00000000e+00,\n",
       "        2.67908364e-03, 0.00000000e+00, 3.12603699e-03, 3.29497294e-03,\n",
       "        2.42062927e-03, 0.00000000e+00, 0.00000000e+00, 3.84906895e-03,\n",
       "        4.25238771e-03, 3.36504502e-04, 1.49807009e-03, 1.47830838e-02,\n",
       "        2.32692284e-03, 2.74297078e-03, 2.19843149e-03, 4.83906532e-03,\n",
       "        5.14993494e-03, 0.00000000e+00, 4.30712874e-03, 4.98357886e-04,\n",
       "        3.32037750e-04, 5.43437102e-04, 0.00000000e+00, 1.20746318e-03,\n",
       "        1.12599290e-02, 3.26814592e-03, 3.07740253e-02, 1.90569381e-03,\n",
       "        4.43917773e-03, 1.84552503e-03, 0.00000000e+00, 2.60688407e-03,\n",
       "        3.48817115e-03, 0.00000000e+00, 4.89535681e-03, 9.85020165e-03,\n",
       "        3.50955044e-04, 0.00000000e+00, 1.92635914e-03, 6.00980248e-03,\n",
       "        0.00000000e+00, 4.63337436e-03, 1.39884666e-03]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_scores_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c792f9",
   "metadata": {},
   "source": [
    "### Step 8: Test the IR System\n",
    "\n",
    "Test the system with a sample query.\n",
    "\n",
    "Retrieve and display the top results using both TF-IDF and BERT representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9562df85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Query Embeddings: [[[-0.45446125]\n",
      "  [ 0.05358833]\n",
      "  [ 0.12024155]\n",
      "  ...\n",
      "  [ 0.07028826]\n",
      "  [ 0.24653077]\n",
      "  [ 0.41181576]]\n",
      "\n",
      " [[-0.25475633]\n",
      "  [-0.07631426]\n",
      "  [ 0.37731266]\n",
      "  ...\n",
      "  [-0.24108677]\n",
      "  [ 0.2728721 ]\n",
      "  [ 0.6000002 ]]\n",
      "\n",
      " [[-0.44223228]\n",
      "  [-0.0525881 ]\n",
      "  [-0.00906704]\n",
      "  ...\n",
      "  [ 0.09634119]\n",
      "  [ 0.16249628]\n",
      "  [ 0.30312762]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.6948799 ]\n",
      "  [-0.00157599]\n",
      "  [ 0.01014499]\n",
      "  ...\n",
      "  [-0.3159717 ]\n",
      "  [ 0.15906921]\n",
      "  [ 0.46990094]]\n",
      "\n",
      " [[-0.26669952]\n",
      "  [ 0.02476714]\n",
      "  [-0.10877821]\n",
      "  ...\n",
      "  [-0.21664155]\n",
      "  [ 0.5348886 ]\n",
      "  [ 0.57949764]]\n",
      "\n",
      " [[-0.6948799 ]\n",
      "  [-0.00157599]\n",
      "  [ 0.01014499]\n",
      "  ...\n",
      "  [-0.3159717 ]\n",
      "  [ 0.15906921]\n",
      "  [ 0.46990094]]]\n",
      "BERT Query Shape: (32, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "bert_embeddings_query = generate_bert_embeddings(query)\n",
    "print(\"BERT Query Embeddings:\", bert_embeddings_query)\n",
    "print(\"BERT Query Shape:\", bert_embeddings_query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18b25b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Query Cosine Similarity:\n",
      " [[1.0000002  0.923278   0.9624414  ... 0.92472047 0.9155723  0.92472047]\n",
      " [0.923278   0.99999976 0.89562345 ... 0.8880375  0.9121578  0.8880375 ]\n",
      " [0.9624414  0.89562345 0.99999994 ... 0.92312145 0.91849136 0.92312145]\n",
      " ...\n",
      " [0.92472047 0.8880375  0.92312145 ... 0.9999999  0.90264225 0.9999999 ]\n",
      " [0.9155723  0.9121578  0.91849136 ... 0.90264225 0.9999999  0.90264225]\n",
      " [0.92472047 0.8880375  0.92312145 ... 0.9999999  0.90264225 0.9999999 ]]\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity between BERT embeddings\n",
    "bert_similarity_query = cosine_similarity(bert_embeddings_query.reshape(32,768))\n",
    "print(\"BERT Query Cosine Similarity:\\n\", bert_similarity_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29236f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la similitud coseno entre cada documento y la query\n",
    "similarities_Bert = cosine_similarity(bert_embeddings.reshape(319,768), bert_embeddings_query.reshape(32,768))\n",
    "\n",
    "# Obtener la similitud promedio por documento\n",
    "avg_similarities_Bert = similarities_Bert.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61bcee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking de documentos (índices): [119 172  16 108 219 128 233  82 237 179  34 310 162 252 120 296  57 106\n",
      " 185 316 147 159 290   5  46  61  62 117 137 184  14  13 114 251  99   3\n",
      " 236 134  84 158 182 271  53  48 225 112 311  42   7  37  11  28 300 209\n",
      "  31  23 165 163 125  87 215 260 289 173 109 194 131  59 189  67 177  43\n",
      " 297 144 268  55   1 245  65 217 266 301 107  20 261 122 267 223  35 295\n",
      "  63 307  54 243 207  98   8 314 101 213 103 302  15 284  52 259 299 118\n",
      "  30  94  24  58 256 239 166 135 133 248 208 281 168  44  80 111 298 202\n",
      " 254  86 171  75 313  27  78  39 258 116 110 105 228   2 102 127  12 274\n",
      " 143 234  83 201 285 187 123 292 247 224 257 221 282 167 129  40 175 222\n",
      "  19 276 249 178 220  70 241  69  60  29 204 309 306  38 206  18   0 286\n",
      " 170 150 138  56  76 199 317 279  92   4 273 265 113 205  45 148 227 255\n",
      " 315 145  89 140  10 104 149 214 169 139 115  81 156  91  85  66  79 176\n",
      " 246 132 235 210   9 141 216  51 124   6 121 157 287 130 154  25 272  26\n",
      "  33  41  32 174 230 308 153 161 212 152  97 232 146 231 318 100 193 263\n",
      " 192 160 197 283 136 151 155  96 264 305 294 275  93 250 253  50 190 196\n",
      "  36 142 203  77  49 280 238 244  68 229  47  64  88  21 291 195 303 242\n",
      "  74  73 164  72  90 269 188 200  22 191 240 278  71 181 288 183 211 198\n",
      "  17 277 262 126 180 304  95 293 226 270 218 312 186]\n",
      "Similitudes de los documentos: [0.61427957 0.6130083  0.61226535 0.61225    0.6094832  0.60934037\n",
      " 0.6075435  0.60653186 0.6063857  0.60376704 0.60289204 0.6016855\n",
      " 0.6016325  0.6011567  0.59940624 0.59871674 0.597938   0.59723896\n",
      " 0.5972248  0.5966289  0.5963218  0.5960036  0.59573877 0.5955656\n",
      " 0.59546787 0.5953286  0.5949532  0.5945156  0.59401333 0.59385204\n",
      " 0.59359646 0.59359646 0.5935323  0.5929606  0.59281313 0.59255314\n",
      " 0.5916072  0.5909935  0.59062827 0.59014916 0.5889465  0.58880496\n",
      " 0.58874035 0.58852357 0.5884692  0.58846205 0.58782136 0.58776104\n",
      " 0.58745974 0.58718544 0.5867404  0.58645546 0.586427   0.58619726\n",
      " 0.5861505  0.5860032  0.585888   0.5856329  0.58477604 0.5846149\n",
      " 0.5843687  0.58426195 0.5840578  0.583559   0.5831861  0.5830278\n",
      " 0.5828099  0.58233565 0.5814956  0.58141357 0.58079326 0.5807929\n",
      " 0.58077276 0.5806992  0.5806906  0.5805312  0.5804675  0.58040285\n",
      " 0.5795733  0.5795343  0.57930017 0.5791533  0.5789462  0.5789352\n",
      " 0.5787326  0.57863414 0.5783864  0.57830197 0.5776855  0.5775343\n",
      " 0.57752806 0.57707    0.57643825 0.5763631  0.5761564  0.57609934\n",
      " 0.57593167 0.5759229  0.57590765 0.57574546 0.5748874  0.574703\n",
      " 0.57375723 0.5734476  0.5724194  0.572214   0.5721222  0.5720881\n",
      " 0.57202864 0.5719918  0.57194245 0.57186365 0.5715874  0.5703905\n",
      " 0.56997144 0.56984866 0.568308   0.5681291  0.5676386  0.56747794\n",
      " 0.56719184 0.5669952  0.56694853 0.5667312  0.5660006  0.56575555\n",
      " 0.56565225 0.56555104 0.565547   0.5653657  0.564764   0.5645871\n",
      " 0.5644017  0.5639937  0.56393164 0.5637877  0.5636618  0.5636008\n",
      " 0.56335664 0.5633266  0.5627304  0.5625919  0.5625702  0.56244224\n",
      " 0.561847   0.56176734 0.5616588  0.5607649  0.56041384 0.5601102\n",
      " 0.5600244  0.55970883 0.5589183  0.55890775 0.5586659  0.558474\n",
      " 0.55822086 0.55782604 0.55772233 0.55702615 0.5567725  0.55668247\n",
      " 0.55667585 0.55663836 0.556321   0.5560018  0.55596733 0.5555322\n",
      " 0.55541164 0.5549634  0.5546458  0.5546291  0.5544384  0.5542042\n",
      " 0.5541264  0.553959   0.5537855  0.5535785  0.55353403 0.55329376\n",
      " 0.5532923  0.5532385  0.55299914 0.5527358  0.55267537 0.55201715\n",
      " 0.5517264  0.5516909  0.55108774 0.5510637  0.5506897  0.5505842\n",
      " 0.5505821  0.55014163 0.550053   0.5500406  0.549536   0.5494782\n",
      " 0.54935575 0.54904795 0.5490187  0.5488323  0.5487283  0.548225\n",
      " 0.54819435 0.5475025  0.54747    0.5470519  0.54675215 0.5465126\n",
      " 0.5463103  0.54615486 0.5456118  0.54517263 0.54497117 0.5447983\n",
      " 0.54400414 0.54383135 0.5436857  0.5434852  0.5432873  0.54297423\n",
      " 0.5428612  0.54283494 0.5423198  0.5417044  0.54129446 0.5412098\n",
      " 0.54117376 0.5406529  0.5403954  0.5403826  0.54033667 0.54024917\n",
      " 0.5394016  0.53936887 0.5393641  0.53839016 0.5383243  0.53806037\n",
      " 0.53740025 0.536354   0.53630924 0.5360697  0.53601253 0.53598785\n",
      " 0.53587615 0.5353658  0.5353402  0.53520787 0.5349002  0.53344\n",
      " 0.53299016 0.53289706 0.53261435 0.5322877  0.5318041  0.53162897\n",
      " 0.53094864 0.5308837  0.53053826 0.52969897 0.5293577  0.52929604\n",
      " 0.52915263 0.52884597 0.52858436 0.5273665  0.5252714  0.52381325\n",
      " 0.5237597  0.5235288  0.5229536  0.5227339  0.5220042  0.5218408\n",
      " 0.521368   0.52098775 0.5206982  0.52042866 0.52031136 0.5198512\n",
      " 0.51907593 0.51899076 0.51871    0.51838547 0.51633525 0.5161884\n",
      " 0.51561046 0.5151372  0.51465356 0.5137643  0.51372695 0.51307845\n",
      " 0.5126625  0.51240396 0.51204467 0.51200926 0.5114471  0.5111006\n",
      " 0.5102974  0.509256   0.50734085 0.5066503  0.50652254 0.50612044\n",
      " 0.50444746 0.50161135 0.501287   0.49689806 0.49645656 0.49517813\n",
      " 0.49451408 0.4927076  0.48367494 0.48348922 0.47658044 0.46777713\n",
      " 0.45132264]\n"
     ]
    }
   ],
   "source": [
    "# Crear un ranking de los documentos basado en la similitud promedio\n",
    "ranking_Bert = np.argsort(-avg_similarities_Bert)\n",
    "\n",
    "# Imprimir el ranking de los documentos\n",
    "print(\"Ranking de documentos (índices):\", ranking_Bert)\n",
    "print(\"Similitudes de los documentos:\", avg_similarities_Bert[ranking_Bert])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23317a",
   "metadata": {},
   "source": [
    "### Step 9: Compare Results\n",
    "\n",
    "Analyze and compare the results obtained from TF-IDF and BERT representations.\n",
    "\n",
    "Discuss the differences, strengths, and weaknesses of each method based on the retrieval results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b3e65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a934919d95ac2de",
   "metadata": {},
   "source": [
    "## Instructions:\n",
    "\n",
    "* Follow the steps outlined above to implement the IR system.\n",
    "* Run the provided code snippets to understand how each part of the system works.\n",
    "* Test the system with various queries to observe the results from both TF-IDF and BERT representations.\n",
    "* Compare and analyze the results. Discuss the pros and cons of each method.\n",
    "* Document your findings and any improvements you make to the system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
